* nanoGPT-PS2
A fork of karpathy's [[https://github.com/karpathy/nanoGPT][nanoGPT]] trained on Kalki's [[https://www.projectmadurai.org/pm_etexts/utf8/pmuni0169_00.html][Ponniyin Selvan]], thanks to [[https://www.projectmadurai.org][Project Madurai]].

#+CAPTION: nanoGPT model that generates ponniyinselvan like text
#+NAME:   fig:ps2-poster
[[./assets/ps2-poster.jpg]]

** Install

Dependencies:
#+begin_src bash
  pip install -r requirements.txt
#+end_src

** quick start
Download [[https://drive.google.com/file/d/1vtTDRtBrl76AUm_f4eeG8Svkpo0wnJ46/view?usp=sharing][Model weights and meta files]] and extract them into their place.

#+begin_src bash
  python sample.py --out_dir=out-ponniyinselvan-char --device=cpu
#+end_src

If you want to type the few initial words and let the model generate the rest use the following. Wait for the =>>>= to appear and then types a couple of words and press return.
#+begin_src bash
    $ python sample-ponni.py --out_dir=out-ponniyinselvan-char  --device=cpu
#+end_src


** Train your own model
If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Kalki's Ponniyin Selvan.
The data is provided with the repository, under data/ponniyinselvan_char/input.txt

#+begin_src bash
  python data/ponniyinselvan_char/prepare.py
#+end_src

This creates a =train.bin= and =val.bin= in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:

**I have a GPU**. Great, we can quickly train a baby GPT with the settings provided in the [[config/train_ponniyinselvan_char.py][config/train_ponniyinselvan_char.py]] config file:

#+begin_src bash
  python train.py config/train_ponniyinselvan_char.py  --compile=False
#+end_src

If you peek inside it, you'll see that we're training a GPT with a context size of up to 256 characters, 768 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one 3060 GPU this training run takes about 40 minutes and the best validation loss is 0.9281. Based on the configuration, the model checkpoints are being written into the =--out_dir= directory =out-ponniyinselvan-char=. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:

#+begin_src bash
  python sample.py --out_dir=out-ponniyinselvan-char --device=cpu
#+end_src

This generates a few samples, for example:

#+begin_quote

"அப்பனே! நான் இப்போது பார்க்கக் கூடிய காரியம் எனக்குத் தெரியும். எனக்கு இச்செய்தி தெரியும் அல்லவா? அது என்ன அதிசயம்?"

"என்னைக் காட்டிலும் இது ஒரு பழைய பைத்தியம்? பாய்மரம் ஒன்று இருக்கிறது! அதைக் கண்டு எனக்கு உயிர் உள்ளானது என்ன யார் என்று தெரியாது" என்றாள்.

"இப்போது என்ன யோசனை சொல்கிறாய்? என் பாட்டனார் இருக்கிறார் போலிருக்கிறது. இருக்கக் கூடும்!" என்று நந்தினி கூறி, வாய்க்கால் போன்ற விளக்கை நோக்கி வாரிப் போட்டுக் கண் விழித்தாள்.

"தேவி! நான் என்னை எப்படியாவது பார்த்துச் சொல்வதில்லை என்று நீயே
#+end_quote

If you want to type the few initial words and let the model generate the rest use the following. Wait for the =>>>= to appear and then types a couple of words and press return.
#+begin_src bash
  $ python sample-ponni.py --out_dir=out-ponniyinselvan-char  --device=cpu

  Overriding: out_dir = out-ponniyinselvan-char
  Overriding: device = cpu
  number of parameters: 42.53M
  Loading meta from data/ponniyinselvan_char/meta.pkl...
  >>> செந்தமிழ் 
  செந்தமிழ் எடுத்துக்கொண்டிருந்தான் நம் கண்டரிதானுடைய பின்தொதியை அடைந்து விடுகிறது. சுற்றிக் கொண்டே முன் மறைந்து விட்டது; அது என்ன?" என்று சொல்லிவிட்டுச் சொல்லி விட்டுப் பூங்குழலி மறுபடியும் அறியாது.

  ஆதித்த கரிகாலனாகப் பிறந்த பிறகு கண்கள் பேய்ந்து மேலும் குந்தவைசேவர்களைக் காஷ்பிட்டுக் கொண்டுபிடித்து வந்தது. அதோ, அப்படியெல்லாம் முன்னலே குறித்து விட்டன. அவள் அந்த வீடு இருவரும் மதில் விழுந்து வைத்துக் கொண்டிருந்தது. மற்றும் ஒரு மூடப்போக்கிரமதி என்பதை நாம் காடுகிறாள் அறியாவிட்டு விடுகிறேன். அவளுடைய உள்ளம் எதிர
  ---------------
  >>> ^CTraceback (most recent call last):
    File "sample-ponni.py", line 82, in <module>
      start = input(">>> ")
  KeyboardInterrupt

#+end_src


**I only have a macbook** (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly [[https://pytorch.org/get-started/locally/][select it here]] when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:

#+begin_src bash
      python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
#+end_src

Here, since we are running on CPU instead of GPU we must set both =--device=cpu= and also turn off PyTorch 2.0 compile with =--compile=False=. Then when we evaluate we get a bit more noisy but faster estimate (=--eval_iters=20=, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with =--lr_decay_iters=). Because our network is so small we also ease down on regularization (=--dropout=0.0=). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:) 

